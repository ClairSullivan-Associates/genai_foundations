{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9915eb4-6a2d-4877-b31b-4845eddd5827",
   "metadata": {},
   "source": [
    "# Module 2, Activity 2: Parameter Tuning for Model Behavior\n",
    "\n",
    "We have thus far seen how to tune the prompt a bit.  Now let's look at ways to tune the model.  We are going to explore three different hyperparameters: temperature, top_p, and the number of tokens.  Please be aware that there are many others that you are encouraged to explore on your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d98b031-3701-462f-8aa0-6147092e034c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "import boto3\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_aws import ChatBedrock, ChatBedrockConverse\n",
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "from langchain_core.runnables import RunnableLambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728c4059-c9ef-49f6-8f99-5012e1938f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.session.Session()\n",
    "region = session.region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d971a0b-f3a9-45d2-8a61-e32f2c7c5aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_s3(bucket_name, key):\n",
    "    s3 = boto3.client(\n",
    "        's3',\n",
    "        region_name=region,\n",
    "    )\n",
    "    response = s3.get_object(Bucket=bucket_name, Key=key)\n",
    "    data = response['Body'].read().decode('utf-8')\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9589ed-02cf-483d-a03d-16077813f424",
   "metadata": {},
   "source": [
    "## Temperature\n",
    "\n",
    "We have briefly touched on temperature but not really gone into too much depth yet.  Think of temperature in an LLM like a \"creativity dial\" for its responses.\n",
    "\n",
    "- Low temperature (e.g., 0.0-0.3): The model plays it safe, sticking to the most likely answers. This is great when you want accuracy and consistency, like coding help or fact-based answers.\n",
    "\n",
    "- High temperature (e.g., 0.7-1.0): The model gets more adventurous, picking less common words and generating more diverse responses. This is useful for creative writing, brainstorming, or when you want unique outputs.\n",
    "\n",
    "If you set it to 0, the model is basically deterministic—it’ll always give the same answer if asked the same thing.  As you increase temperature you will get more creative (and perhaps unpredictable!) responses.  So let's create a simple prompt and run it a few times.  First we will set a low temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe499525-ca82-494c-9031-dc0a15dc3386",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = SystemMessagePromptTemplate.from_template(\n",
    "    \"You are a helpful assistant.\"\n",
    ")\n",
    "human_prompt = HumanMessagePromptTemplate.from_template(\n",
    "    \"{input}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([system_prompt, human_prompt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5288b47-6a94-432d-9839-c8ba19cf5c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatBedrock(\n",
    "    model_id=\"amazon.titan-text-express-v1\",\n",
    "    region_name=region,\n",
    "    temperature=0.0,\n",
    "    max_tokens=1000,\n",
    ")\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8b1389-8b7a-4ec7-aaa5-5d7d5639ed3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke(\"Describe the plot of Hamlet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd4d341-b940-48d7-8586-93a95e64fb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke(\"Describe the plot of Hamlet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd353bb-4e41-4b55-91c9-059054572907",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke(\"Describe the plot of Hamlet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482ba714-60f2-4045-949e-3c9b00c0a6d2",
   "metadata": {},
   "source": [
    "Because the temperature is low you will not see much variation between each of these three runs.  Now let's turn it up to a high value and see what happens with multiple runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79c7391-7f1d-4161-bf33-426eb01243ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatBedrock(\n",
    "    model_id=\"amazon.titan-text-express-v1\",\n",
    "    region_name=region,\n",
    "    temperature=0.9,\n",
    "    max_tokens=1000,\n",
    ")\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86da5b92-dc0f-44bf-bc3d-890cc5f736c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke(\"Describe the plot of Hamlet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e859f0-afdb-4e8d-a25e-e2b3a54e7994",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke(\"Describe the plot of Hamlet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2337bac5-ce5d-4484-bad5-a84eee807190",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke(\"Describe the plot of Hamlet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e7ef8b-5bb5-4323-9ab6-ad2cced4adcc",
   "metadata": {},
   "source": [
    "What did you notice when you did this?  Hopefully you see much more variability.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3867531b-afe0-472a-9972-98e503cfc55f",
   "metadata": {},
   "source": [
    "## ChatBedrockConverse()\n",
    "\n",
    "As the LLM space evolves, new APIs are always coming out that add more functionality.  In particular, we are now going to tune a different parameter in the LLM that is not part of the original `ChatBedrock`.  So we will move to `ChatBedrockConverse`, which is the Bedrock chat model integration built on the Bedrock converse API.  This implementation will eventually replace the existing ChatBedrock implementation once the Bedrock converse API has feature parity with older Bedrock API.  Specifically the converse API does not yet support custom Bedrock models.\n",
    "\n",
    "## top_p (AKA nucleus sampling)\n",
    "\n",
    "We are now going to turn to a different hyperparameter we can tune called top_p or nucleus sampling.  Top_p is a way to narrow down the choices the model can pick from when generating text. Instead of considering every possible word, it focuses on a small group of the most likely words that together make up a certain percentage of the chance to appear. This helps keep the response creative yet sensible by avoiding too many unlikely word choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426b813f-1c62-4a01-9ce0-e5dd50666631",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatBedrockConverse(\n",
    "    model=\"anthropic.claude-3-sonnet-20240229-v1:0\",  # Note that not all models support top_p\n",
    "    temperature=0.0,\n",
    "    top_p=0.9,\n",
    ")\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d80690-924c-4207-9ae5-0739eeb89f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke(\"Compose a sonnet about my love of coffee.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a958372-e1aa-41ac-8606-da7c80b56e45",
   "metadata": {},
   "source": [
    "Now let's change the value of top_p..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fad583b-0901-4ce8-9812-4af737425587",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatBedrockConverse(\n",
    "    model=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    temperature=0.0,\n",
    "    top_p=0.2,\n",
    ")\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b771f645-c045-4d96-b9ab-d6acf3dd55e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke(\"Compose a sonnet about my love of coffee.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4a332a-306f-4323-83f0-852fc0b33e63",
   "metadata": {},
   "source": [
    "## Updating both at the same time\n",
    "\n",
    "When you adjust both temperature and top_p at the same time, you're fine-tuning two different aspects of the sampling process:\n",
    "\n",
    "**Temperature:**\n",
    "This parameter scales the raw logits (the model’s confidence levels for each token) before sampling.\n",
    "\n",
    "- Low Temperature: Leads to a peaked distribution where the model is more confident in its top choices, resulting in more deterministic outputs.\n",
    "- High Temperature: Flattens the distribution, introducing more randomness and potentially more creative outputs.\n",
    "  \n",
    "**top_p:**\n",
    "This parameter restricts the sampling to only the smallest set of tokens whose cumulative probability exceeds the threshold p.\n",
    "\n",
    "- Low top_p (e.g., 0.3): Limits the model to only the most likely tokens, which makes the output more focused and less varied.\n",
    "- High top_p (e.g., 0.9): Allows a wider selection of tokens, increasing diversity in the output.\n",
    "  \n",
    "**Combined Effects:**\n",
    "\n",
    "- If you set a high temperature and a high top_p, the model is encouraged to explore a wide range of tokens, resulting in very creative, diverse, and potentially less predictable responses.\n",
    "- Conversely, a low temperature with a low top_p will constrain the model to the most likely tokens, leading to more consistent and deterministic outputs.\n",
    "- Tuning both parameters simultaneously lets you balance the trade-off between creativity and predictability in the model’s output. Experimentation is key to finding the right mix for your particular use case.\n",
    "\n",
    "Be sure to play with your own questions and combinations of temperature and top_p to see what you get!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144dc906-ab5b-4695-a1a5-3a9e365f6b06",
   "metadata": {},
   "source": [
    "## Tokens\n",
    "\n",
    "At this point we have been specifying the maximum number of tokens that a model can return.  Each model has its own limit on the maximum total number of tokens it will work with (either on input and/or output), called the \"context window\" or \"context size\".  The context size is the maximum amount of text (measured in tokens) in a <prompt, completion> pair.  For the models we are working with, here is that value:\n",
    "\n",
    "- Titan Text G1 Lite: 4k\n",
    "- Titan Text G2 Express: 8k\n",
    "- Claude 3 Sonnet: 200k (maximum output: 8k)\n",
    "- Claude 3 Haiku: 200k (maximum output: 4k)\n",
    "\n",
    "Let's see this in action now by working with data that will not all necessarily fit into the context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d59eb57-880f-40b2-bab4-770e5e03eda5",
   "metadata": {},
   "source": [
    "## Note on throttling\n",
    "\n",
    "Each model has a rate limit associated with it and those vary model by model.  If start running many back-to-back queries and receive this error\n",
    "```\n",
    "ERROR:root:Error raised by bedrock service: An error occurred (ThrottlingException) when calling the InvokeModel operation (reached max retries: 4): Too many requests, please wait before trying again. You have sent too many requests.  Wait before trying again.\n",
    "```\n",
    "either try a different model or wait a few minutes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e791a038-c581-42e9-9905-44a77032794c",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_data = get_data_from_s3(\"bucket-test-cj\", \"hamlet.txt\")\n",
    "s3_data[0:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9532f1-63e9-4b87-b2cf-75972bb576db",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatBedrockConverse(\n",
    "    model=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    ")\n",
    "\n",
    "token_count = llm.get_num_tokens(s3_data)\n",
    "print(token_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69eb4f94-3afc-4adc-998a-146c6af4bd52",
   "metadata": {},
   "source": [
    "Let's start by seeing what happens if we run this through a model that does not have a large enough context size to accommodate the data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a626690-2c79-4b24-8d6a-8379c0d9fc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = SystemMessagePromptTemplate.from_template(\n",
    "    \"You are a helpful assistant.\"\n",
    ")\n",
    "human_prompt = HumanMessagePromptTemplate.from_template(\n",
    "    \"{input}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([system_prompt, human_prompt])\n",
    "\n",
    "llm = ChatBedrock(\n",
    "    model_id=\"amazon.titan-text-express-v1\",\n",
    "    region_name=region,\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b696254-fdfe-4a7d-93d5-861b83d31b47",
   "metadata": {},
   "source": [
    "At best, this will happen..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2300258d-3833-4951-98cd-c67670919632",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "chain.invoke(f\"What happens at the end of {s3_data}?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034124bc-3dfa-4a89-b6cc-f1f1f1f377b9",
   "metadata": {},
   "source": [
    "(Note that that message is in characters (s3_data + prompt) and not tokens.)  At worst, you will not be given a similar message and just think that you have fully gotten all of the data in only to be given a very wrong answer.\n",
    "\n",
    "Now let's try this with a better model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffdb96f-6345-4b4c-851e-7a9f58e45c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = SystemMessagePromptTemplate.from_template(\n",
    "    \"You are a helpful assistant.\"\n",
    ")\n",
    "human_prompt = HumanMessagePromptTemplate.from_template(\n",
    "    \"{input}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([system_prompt, human_prompt])\n",
    "\n",
    "llm = ChatBedrock(\n",
    "    model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    region_name=region,\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab14dba4-63a0-4ea9-b2d4-73b7ae04bb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke(f\"What happens at the end of {s3_data}?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a434d70-cf00-48bd-8a8b-3bee999d69b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
