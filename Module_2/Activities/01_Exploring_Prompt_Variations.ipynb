{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d78cb6b-99c1-4617-8bc6-957542193e46",
   "metadata": {},
   "source": [
    "# Module 2, Activity 1: Exploring Prompt Variations\n",
    "\n",
    "When it comes to creating AI applications, the secret sauce is in the prompt.  Prompt engineering is as much of an art as it is a science.  But you will see in this notebook that there are some rules you can follow to write effective prompts.  We begin by loading up our typical code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14c2c73-3321-40ee-b13e-6473a5996578",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "import boto3\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_aws import ChatBedrock\n",
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "from langchain_core.runnables import RunnableLambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b007f460-a050-4107-a648-735c7c9b5082",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.session.Session()\n",
    "region = session.region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5d2bd9-5272-4e18-9d93-3c4ba980bbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_s3(bucket_name, key):\n",
    "    s3 = boto3.client(\n",
    "        's3',\n",
    "        region_name=region,\n",
    "    )\n",
    "    response = s3.get_object(Bucket=bucket_name, Key=key)\n",
    "    data = response['Body'].read().decode('utf-8')\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32f0a78-83b2-4c0d-99ae-c48545f6482f",
   "metadata": {},
   "source": [
    "## Analyzing text\n",
    "\n",
    "We are going to focus on analyzing raw text, however, what we learn below will be applicable to any of the three tracks for this course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da61270-83a1-4b1d-95e7-371bfe46d3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_data = get_data_from_s3(\"...\", \"constitution.txt\")\n",
    "s3_data[0:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438ccb43-332b-4ff8-a44d-a759d1c2b1ca",
   "metadata": {},
   "source": [
    "## Create our basic chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3052db-c288-49bd-acdd-9080d574a4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = SystemMessagePromptTemplate.from_template(\n",
    "    \"You are a helpful assistant.\"\n",
    ")\n",
    "human_prompt = HumanMessagePromptTemplate.from_template(\n",
    "    \"{input}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([system_prompt, human_prompt])\n",
    "\n",
    "llm = ChatBedrock(\n",
    "    model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    region_name=region,\n",
    "    temperature=0.5,\n",
    "    max_tokens=1000,\n",
    ")\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07b1198-1ca9-4850-a644-ea3c67a90566",
   "metadata": {},
   "source": [
    "## Zero-Shot Prompting\n",
    "\n",
    "The most basic type of prompting is called \"zero-shot prompting,\" which is when you provide the model with just the question or instruction and expect it to get the answer just based on its pre-trained knowledge.  No examples or additional context are provided.  Let's try it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576a8ee6-850d-41ac-a0b2-7818f49c8461",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_shot_prompt = f\"Provide an analysis of {s3_data}\"\n",
    "\n",
    "chain.invoke({\"input\": zero_shot_prompt})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7f10eb-af82-4e78-9c49-4f73c9d76bbb",
   "metadata": {},
   "source": [
    "## Few-Shot Prompting\n",
    "\n",
    "While that did alright, we can do better if we can provide a few examples within the prompt to guide the model's behavior.  This is called \"few-shot prompting.\"  By demonstrating the format, style, or type of answer you expect, the model can better understand and mimic that structure in its response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70d369d-1c56-445f-af54-1024ff31d851",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_prompt = f\"\"\"\n",
    "    You are provided with {s3_data} and given the following:\n",
    "    Example 1:\n",
    "    Q: What is the significance of the Constitution of the United States?\n",
    "    A: The Constitution is the supreme law of the United States, establishing the framework for the federal government and protecting individual rights.\n",
    "    \n",
    "    Example 2:\n",
    "    Q: How does the Constitution implement checks and balances?\n",
    "    A: It divides power among three branches—executive, legislative, and judicial—ensuring that no single branch dominates the government.\n",
    "    \n",
    "    Now, answer the following:\n",
    "    Q: Provide an analysis of the Constitution of the United States.\n",
    "    A:\n",
    "\"\"\"\n",
    "\n",
    "chain.invoke({'input': few_shot_prompt})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aefd28e-793f-4524-ae22-5449df6af283",
   "metadata": {},
   "source": [
    "## Chain-of-Thought (COT) Prompts\n",
    "\n",
    "Chain-of-thought prompting is a technique where you guide the model to break down its reasoning process into sequential steps before arriving at the final answer. Instead of generating a direct answer in one go, you instruct the model to \"think aloud\" by detailing intermediate steps.  This can lead to more thorough and accurate responses, especially for complex or multi-step problems.  It works particularly well with more sophisticated models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cfa807-71ea-4c6c-bcf9-9d6b305f86a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_of_thought_prompt = f\"\"\"\n",
    "You are an expert in constitutional law.  You have been provided {s3_data}.\n",
    "Please analyze this data by following these steps:\n",
    "Step 1: Summarize the primary purpose and structure of the Constitution.\n",
    "Step 2: Explain the system of checks and balances and why it is crucial.\n",
    "Step 3: Discuss how the Constitution has influenced modern governance and legal interpretations.\n",
    "Provide your detailed analysis:\n",
    "\"\"\"\n",
    "\n",
    "chain.invoke({'input': chain_of_thought_prompt})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e472afa5-b594-4858-a221-f10afb16e6da",
   "metadata": {},
   "source": [
    "## Best Practices for Prompt Engineering\n",
    "\n",
    "Notice that we used statements like \"you are an expert in...\"  Informing the LLM how they should respond within the prompt is considered to be good prompt engineering practice.  But there are many other things you should consider adding to your prompts.  Here are some general guidelines taken from [this website](https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-the-openai-api):\n",
    "\n",
    "- Use the latest models (noting that many popular models are updated regularly...be sure you have the most recent version)\n",
    "- Put the instructions at the beginning of the prompt and use delimiters like `####` or `\"\"\"\"` to separate the instruction and context.\n",
    "- Be specific, descriptive, and as detailed as possible about the desired context, outcome, length, format, style, etc.\n",
    "- Articulate the desired output format through examples\n",
    "- When possible, do not use imprecise descriptions\n",
    "- Don't just say what NOT to do...say what to do instead\n",
    "\n",
    "Let's now give some of these a try with a different problem.  We will start by refreshing our chain..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8e9967-5768-48c1-9f14-79d9e5d54004",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = SystemMessagePromptTemplate.from_template(\n",
    "    \"You are a helpful assistant.\"\n",
    ")\n",
    "human_prompt = HumanMessagePromptTemplate.from_template(\n",
    "    \"{input}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([system_prompt, human_prompt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b42536a-74bf-4dd3-ac9c-d1ed767f80da",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatBedrock(\n",
    "    model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    region_name=region,\n",
    "    temperature=0.5,\n",
    "    max_tokens=1000,\n",
    ")\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01728386-8f5f-41d8-854d-1dbcccd8d989",
   "metadata": {},
   "source": [
    "## Experimenting with Prompts\n",
    "\n",
    "Below are prompts all asking about the same thing but worded differently.  Please experiment with them to see the types of answers you get and try to make them better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4e53cc-25ea-498b-9b6b-e77fa3e8ba4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_shot_prompt = \"Explain the benefits of using generative AI in software development.\"\n",
    "\n",
    "few_shot_prompt = \"\"\"\n",
    "    You will be provided some example questions after the #### delimiter.  You will then be asked a question after the next\n",
    "    #### delimeter.  Answer the question in two sentences.\n",
    "\n",
    "    ####\n",
    "    Example 1:\n",
    "    Q: What is generative AI?\n",
    "    A: Generative AI is a type of artificial intelligence that can create new content by learning from a dataset.\n",
    "    \n",
    "    Example 2:\n",
    "    Q: How can generative AI assist software engineers?\n",
    "    A: It can automate repetitive coding tasks and generate documentation.\n",
    "\n",
    "    ####\n",
    "    Now answer:\n",
    "    Q: Explain the benefits of using generative AI in software development.\n",
    "    A:\n",
    "\"\"\"\n",
    "\n",
    "chain_of_thought_prompt = \"\"\"\n",
    "    You are a senior software engineer.  You will be provided a question and some steps to follow after the #### delimeter.\n",
    "    Provide your answer with minimal jargon like you are explaining it to a 14 year old.  Your total answer should not go\n",
    "    over 3 benefits with a sentence explaining each.\n",
    "    ####\n",
    "    Please explain the benefits of using generative AI in software development by breaking your response into steps:\n",
    "    Step 1: List the main benefits.\n",
    "    Step 2: Explain why each benefit is important.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86df5dd-b34a-4e3f-b4f3-14c3652615c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({\"input\": zero_shot_prompt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fe0472-bd58-44b2-9507-3edd8f73c0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({\"input\": few_shot_prompt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af168a35-d80d-4a2d-bb11-0ce7033bdb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({\"input\": chain_of_thought_prompt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb3ac8d-ca5c-477c-93ee-1be5c2cf6355",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
