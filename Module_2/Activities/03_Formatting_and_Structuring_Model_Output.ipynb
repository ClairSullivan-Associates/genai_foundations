{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcffb15e-7410-4de3-abca-54042b933c8d",
   "metadata": {},
   "source": [
    "# Module 2, Activity 3: Formatting and Structuring Model Output\n",
    "\n",
    "We briefly looked at how to create structured output in a previous activity.  In this notebook we are going to look at this in more detail, along with how (and when) to create custom parsers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8f1065-833a-4bb0-876e-d2658d4b2775",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import YamlOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8570882-3f9a-4fca-87cd-3974a3fe1817",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "import boto3\n",
    "import re\n",
    "import yaml\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_aws import ChatBedrock, ChatBedrockConverse\n",
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser, BaseOutputParser\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain.output_parsers import YamlOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613ab17a-eb7a-4b8c-ac82-1007e77402e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.session.Session()\n",
    "region = session.region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef654a39-753f-4980-9d37-5c40f1070c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_s3(bucket_name, key):\n",
    "    s3 = boto3.client(\n",
    "        's3',\n",
    "        region_name=region,\n",
    "    )\n",
    "    response = s3.get_object(Bucket=bucket_name, Key=key)\n",
    "    data = response['Body'].read().decode('utf-8')\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a96139-4083-4736-960f-23d5b64bd41d",
   "metadata": {},
   "source": [
    "## Reviewing JSON parsers\n",
    "\n",
    "We saw the `JsonOutputParser` in a previous activity.  If we can get the LLM to return proper JSON as a string, then this is a simple way to get that output formatted in true JSON.  However, be sure to read this system prompt and observe how particular we are being with regard to what the output is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da412bf6-6b5f-4650-af72-e1ed709ba118",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = SystemMessagePromptTemplate.from_template(\n",
    "    f\"\"\"You are a helpful assistant. Please provide an analysis of the benefits of structured outputs in AI applications.\n",
    "    Return ONLY your answer in valid JSON format with the following keys: \n",
    "    \"summary\": A brief summary of the analysis,\n",
    "    \"benefits\": a list of benefits, and \n",
    "    \"recommendations\": a list of recommendations.\n",
    "    \"\"\"\n",
    ")\n",
    "human_prompt = HumanMessagePromptTemplate.from_template(\n",
    "    \"Respond to the question.\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([system_prompt, human_prompt])\n",
    "\n",
    "# Initialize the Bedrock LLM.\n",
    "llm = ChatBedrock(\n",
    "    model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    region_name=region,  # Ensure that `region` is defined in your environment.\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "# Create the chain and invoke it.\n",
    "chain = prompt | llm | JsonOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3461413-2d7f-44b9-8d6d-ec3aa5df005d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8026540-b20f-48dd-b0d1-047ceec74398",
   "metadata": {},
   "source": [
    "## Making it more complex\n",
    "\n",
    "That was a pretty simple output so the `JsonOutputParser()` didn't have (or shouldn't have had!) too many problems.  But that is why it was important that we were specific in the prompt about \"ONLY\" outputting the answer in valid JSON.  LLMs frequently will try to be \"too helpful\" and return additional text beyond the JSON.  Also, if the data or prompt get more complicated, the LLM is likely to start outputting more characters and that will really confuse this parsers because it only accepts a very specific format.\n",
    "\n",
    "So let's actually make it harder... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b44f6a-d23f-4221-8a1c-60cd69e1fbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_data = get_data_from_s3(\"bucket-test-cj\", \"hamlet.txt\")\n",
    "s3_data[0:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e66cb4-efca-4069-b130-52492f057ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = SystemMessagePromptTemplate.from_template(\n",
    "    \"\"\"You are a helpful assistant. You will be provided a script.\n",
    "    Identify the following in the script and output EXACTLY a valid JSON object with no additional text:\n",
    "    {{\"characters\": [list of character names], \"locations\": [list of location names]}}\n",
    "    For example, if the script mentions characters \"Hamlet\" and \"Horatio\" and locations \"Denmark\" and \"Norway\", output exactly:\n",
    "    {{\"characters\": [\"Hamlet\", \"Horatio\"], \"locations\": [\"Denmark\", \"Norway\"]}}\n",
    "    Output ONLY the JSON object and nothing else.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "human_prompt = HumanMessagePromptTemplate.from_template(\n",
    "    \"Here is the script: {script}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([system_prompt, human_prompt])\n",
    "\n",
    "# Initialize the Bedrock LLM.\n",
    "llm = ChatBedrock(\n",
    "    model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    region_name=region,\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "# Create the chain and invoke it.\n",
    "chain = prompt | llm | JsonOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93159e34-8a68-4924-b5e1-8fc337bcd900",
   "metadata": {},
   "source": [
    "Now we are going to try and run all of this text through the chain, which is very likely going to result in the LLM not following the system prompt to the letter.  Don't worry if you get an error when you run `chain.invoke(...)`.  Just check out what the error says."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e256dc-184f-4094-ba35-f6869cdc6686",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({\"script\": s3_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0441b26-685d-4f87-89a5-60417b06a00a",
   "metadata": {},
   "source": [
    "## Custom parsers\n",
    "\n",
    "You can create a new class inherited from LangChain's `BaseOutputParser` class to handle whatever life may throw at you in outputs.  Here is on that will use regex to get rid of anything that might not be formatted like proper JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd09196-5eb3-4415-9df6-c3ee6e3a76a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomJsonOutputParser(BaseOutputParser):\n",
    "    def parse(self, text: str):\n",
    "        # Use regex to capture the first JSON object in the text.\n",
    "        json_match = re.search(r'\\{.*\\}', text, re.DOTALL)\n",
    "        if not json_match:\n",
    "            raise ValueError(\"No JSON object found in the output.\")\n",
    "        json_str = json_match.group(0)\n",
    "        try:\n",
    "            return json.loads(json_str)\n",
    "        except json.JSONDecodeError as e:\n",
    "            raise ValueError(f\"Error decoding JSON: {e}\")\n",
    "\n",
    "    def get_format_instructions(self) -> str:\n",
    "        return \"Output should be a valid JSON object.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9014189f-bc20-427d-950b-389c99bd8924",
   "metadata": {},
   "source": [
    "## Modifying the system prompt and chain\n",
    "\n",
    "Notice that we are now providing a few-shot prompt (with double `{{` and `}}` so it understands what is JSON versus a variable).  We also have replaced the original parser with the above custom one in the chain.  This should make it more robust to additional text the LLM might decide to throw in (even thought it has been told not to)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed097b0a-0774-465a-a82d-ad35e8829a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = SystemMessagePromptTemplate.from_template(\n",
    "    \"\"\"You are a helpful assistant. You will be provided a script.\n",
    "    Identify the following in the script and output EXACTLY a valid JSON object with no additional text:\n",
    "    {{\"characters\": [list of character names], \"locations\": [list of location names]}}\n",
    "    For example, if the script mentions characters \"Hamlet\" and \"Horatio\" and locations \"Denmark\" and \"Norway\", output exactly:\n",
    "    {{\"characters\": [\"Hamlet\", \"Horatio\"], \"locations\": [\"Denmark\", \"Norway\"]}}\n",
    "    Output ONLY the JSON object and nothing else.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "human_prompt = HumanMessagePromptTemplate.from_template(\n",
    "    \"Here is the script: {script}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([system_prompt, human_prompt])\n",
    "\n",
    "# Initialize the Bedrock LLM.\n",
    "llm = ChatBedrock(\n",
    "    model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    region_name=region,\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "# Create the chain and invoke it.\n",
    "chain = prompt | llm | CustomJsonOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04eae6ec-3dcf-4d0c-926e-81b537d1dc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({\"script\": s3_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcc4f0a-01bd-4a01-86af-03a54aa7ecb0",
   "metadata": {},
   "source": [
    "## YAML\n",
    "\n",
    "Let's make this a little more interesting by doing this a different way.  We are now going to output YAML (again, creating a custom class beyond the LangChain `YamlOutputParser`, which like the JSON parser has a hard time when LangChain returns text it shouldn't (even though we asked nicely).  But now we are creating a class with Pydantic to show what we are specifically looking for out of the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646b97b2-704f-41d4-80dd-98f8b43af4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
    "\n",
    "\n",
    "class CustomYamlOutputParser(BaseOutputParser):\n",
    "    pydantic_object: type[BaseModel] = Field(...)\n",
    "\n",
    "    def clean_yaml_text(self, text: str) -> str:\n",
    "        # Remove markdown code fences and optional 'yaml' language tag.\n",
    "        text = re.sub(r\"^```(?:yaml)?\\s*\", \"\", text)\n",
    "        text = re.sub(r\"\\s*```$\", \"\", text)\n",
    "        return text.strip()\n",
    "\n",
    "    def parse(self, text: str):\n",
    "        try:\n",
    "            clean_text = self.clean_yaml_text(text)\n",
    "            data = yaml.safe_load(clean_text)\n",
    "            return self.pydantic_object.parse_obj(data)\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error parsing YAML: {e}\")\n",
    "\n",
    "    def get_format_instructions(self) -> str:\n",
    "        return \"Output should be valid YAML corresponding to the pydantic model.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21808d54-4a7c-4bac-950c-c6bc092d779c",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = SystemMessagePromptTemplate.from_template(\n",
    "    \"\"\"You are a comedian. When given a subject, create a joke in YAML format with exactly two keys:\n",
    "    setup: The setup or question for the joke.\n",
    "    punchline: The punchline that resolves the joke.\n",
    "    For example, if the subject is robots, your output should look exactly like:\n",
    "    setup: \"Why did the robot cross the road?\"\n",
    "    punchline: \"To get to the other side!\"\n",
    "    Output ONLY the YAML and nothing else.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "human_prompt = HumanMessagePromptTemplate.from_template(\n",
    "    \"Create a joke about {subject}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([system_prompt, human_prompt])\n",
    "\n",
    "llm = ChatBedrock(\n",
    "    model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    region_name=region,\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "chain = prompt | llm | CustomYamlOutputParser(pydantic_object=Joke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ca6418-2b4f-4184-83be-eb32a4613da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({\"subject\": \"robots\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abc5d28-6053-4efa-aa54-753d087ae17e",
   "metadata": {},
   "source": [
    "## Bottom line\n",
    "\n",
    "There are many different output parsers for LangChain.  However, they are only as good as what comes out of the LLM.  As we have seen here, frequently the LLM output does not line up with what these strict parsers want as input.  Therefore, creating custom parsers is the way to go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6802bb0f-04c1-4c2a-8b0c-92a142c40ded",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
