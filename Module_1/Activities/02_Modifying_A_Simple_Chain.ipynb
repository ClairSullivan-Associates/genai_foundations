{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "RtoVGkSh2T_i",
        "outputId": "e1235775-63bb-49af-ca5e-21bc1f627c77"
      },
      "outputs": [],
      "source": [
        "!pip install boto3\n",
        "!pip install langchain\n",
        "!pip install langchain_aws"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jV7Rg5p8dVDr"
      },
      "source": [
        "# Module 1, Activity 2: Modifying a Simple Chain\n",
        "\n",
        "In this notebook we will learn about how to create chains in LangChain and make them more sophisticated through the use of prompt templates and memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nBtgPEN22bKV"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from pprint import pprint\n",
        "\n",
        "from langchain.chains import LLMChain\n",
        "from langchain_aws import ChatBedrock\n",
        "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
        "from langchain_core.runnables import RunnableLambda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uIE8etrk3RZ4"
      },
      "outputs": [],
      "source": [
        "AWS_DEFAULT_REGION=\"...\"\n",
        "AWS_ACCESS_KEY_ID=\"...\"\n",
        "AWS_SECRET_ACCESS_KEY=\"...\"\n",
        "AWS_SESSION_TOKEN=\"...\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DF5s1fG-Yd48"
      },
      "source": [
        "## Establishing our connection to Bedrock\n",
        "\n",
        "Be sure to pick a model that is already loaded into the Bedrock.  Have fun adjusting `temperature` and `max_tokens.`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iEiIcgc1395Q"
      },
      "outputs": [],
      "source": [
        "llm = ChatBedrock(\n",
        "    model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",  # Replace with your desired model ID\n",
        "    region_name=AWS_DEFAULT_REGION,\n",
        "    aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
        "    aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n",
        "    aws_session_token=AWS_SESSION_TOKEN,\n",
        "    temperature=0.5,\n",
        "    max_tokens=200,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-iOgjfkYo-W"
      },
      "source": [
        "## Prompts and Prompt Templates\n",
        "\n",
        "In LangChain, **system prompts** (provide the foundational instructions and context for the AI model.  They define the model's role, behavior, and any specific output format or constraints, ensuring that all subsequent responses align with the intended purpose.  In contrast, **human prompts** represent the dynamic, user-supplied inputs that drive the conversation or query.  They capture the specific question or instruction that the user wants the model to address.  The `ChatPromptTemplate` acts as a framework that seamlessly integrates both types of prompts into a coherent message sequence.  By combining system and human messages, the `ChatPromptTemplate` creates a structured dialogue that guides the model to generate responses that are both contextually relevant and properly formatted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NX3E9Fs64MMH"
      },
      "outputs": [],
      "source": [
        "system_prompt = SystemMessagePromptTemplate.from_template(\n",
        "    \"You are a helpful assistant.  Return your answer in pirate speak.\"\n",
        ")\n",
        "human_prompt = HumanMessagePromptTemplate.from_template(\"{input}\")\n",
        "prompt = ChatPromptTemplate.from_messages([system_prompt, human_prompt])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yw4QNdNZQO1"
      },
      "source": [
        "## Chains\n",
        "\n",
        "Chains in LangChain are modular sequences that connect various components—such as prompt templates, language models, and output parsers—into a unified workflow.  They enable developers to construct complex, multi-step processing pipelines where each step transforms or utilizes the output from the previous one.  LCEL (LangChain Chain Expression Language) implements this concept using a concise, pipe operator (|) syntax that clearly defines the data flow between these components.  By leveraging LCEL, you can declaratively compose chains that are both flexible and readable, allowing for rapid prototyping and iterative development of sophisticated AI-powered applications.\n",
        "\n",
        "In this code block we are creating a simple, two-part chain with our prompt and our LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GJ6jdna4TjH",
        "outputId": "a8e6e3ef-f209-4827-f803-6d9833ec0961"
      },
      "outputs": [],
      "source": [
        "chain = prompt | llm\n",
        "chain.invoke(\"What is the recipe for mayonnaise?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYPWEem-ZqtZ"
      },
      "source": [
        "## Output Parsers\n",
        "\n",
        "The `AIMessage` format might not be the best way for the output of your LLM calll to be rendered.  So we can keep adding to the chain by adding another component: `StrOutputParser`.  It converts a language model's output into a clean, plain text string by stripping away any additional metadata or formatting.  So we can see in this example that we just add it modularly to the end of our chain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "fX8EFHSB6C28",
        "outputId": "e349d9da-4254-488d-bb39-65c330cee938"
      },
      "outputs": [],
      "source": [
        "chain = prompt | llm | StrOutputParser()\n",
        "chain.invoke(\"What is the recipe for mayonnaise?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlK-Qv3aaJ8v"
      },
      "source": [
        "## Other Output Parsers\n",
        "\n",
        "There are several different output parsers available in LangChain.  You can read about them all in the [API docs](https://python.langchain.com/api_reference/core/output_parsers.html).  Here is another common use case using the `JsonOutputParser` to get the result of the LLM call in a convenient JSON format.  Note that we changed the system prompt to reflect how we wanted that JSON to be formatted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BlFjOvfoV4ik",
        "outputId": "c3902906-5d5c-4726-803e-0d9084514cfd"
      },
      "outputs": [],
      "source": [
        "system_prompt = SystemMessagePromptTemplate.from_template(\n",
        "    \"You are a helpful assistant. Return your answer in JSON format with exactly two keys: 'response' and 'details'.\"\n",
        ")\n",
        "human_prompt = HumanMessagePromptTemplate.from_template(\"{input}\")\n",
        "prompt = ChatPromptTemplate.from_messages([system_prompt, human_prompt])\n",
        "\n",
        "chain = prompt | llm | JsonOutputParser()\n",
        "\n",
        "json_output = chain.invoke(\"What is the recipe for mayonnaise?\")\n",
        "pprint(json_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkGOC3kZSGpI",
        "outputId": "33450ff8-8c8c-4ee0-a7cd-9042a07ff32a"
      },
      "outputs": [],
      "source": [
        "chain.invoke(\"What recipe did I just ask you for?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkFZuUlval01"
      },
      "source": [
        "## Oops!\n",
        "\n",
        "Notice that we just asked the LLM to tell us about what we have been talking about with it.  However, LLM's do not, by default, have any memory.  This is something we need to add to it by creating a chat history.\n",
        "\n",
        "_**Note:**_\n",
        "\n",
        "It is likely when you run the below cell you will get a deprecation error.  This is because LangChain is changing how they handle conversational memory, encouraging migration to their new platform, LangGraph.  LangGraph is a sophisticated platform used for the orchestration of multi-agent bots that use tools.  At this stage it is beyond the scope of where we are in this workshop but we will discuss it when we get to \"Module 4: Agents and Tool Use.\"  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxmdd_-i4RMZ"
      },
      "outputs": [],
      "source": [
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLF0HO9obp4w"
      },
      "source": [
        "## A node about this function\n",
        "\n",
        "We will now be creating a chain through a different approach than LCEL, which will allow us to add this memory we just created.  However, the output of that is in a different format than the chain created above with LCEL.  So this helper function will be used to convert it back into something LCEL can work with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mKF0Xn95QRfO"
      },
      "outputs": [],
      "source": [
        "def convert_output_to_str(x):\n",
        "\n",
        "    \"\"\"\n",
        "    Recursively converts the input object into a JSON-serializable string.\n",
        "\n",
        "    This function processes the input recursively:\n",
        "    - If the object is a string, it is returned as is.\n",
        "    - If the object has a 'content' attribute (e.g., a HumanMessage), it retrieves the content and processes it recursively.\n",
        "    - If the object is a dictionary, it recursively converts each key-value pair.\n",
        "    - If the object is a list, it recursively converts each element in the list.\n",
        "    - For any other object types, it returns the object unchanged.\n",
        "\n",
        "    After recursive conversion, if the resulting object is a dictionary or list,\n",
        "    it is serialized into a JSON string using `json.dumps`. Otherwise, its string\n",
        "    representation is returned.\n",
        "\n",
        "    This ensures that the output passed to the downstream parser (e.g., StrOutputParser)\n",
        "    is a valid JSON-serializable string.\n",
        "    \"\"\"\n",
        "    def recursive_convert(obj):\n",
        "        # If already a string, return it.\n",
        "        if isinstance(obj, str):\n",
        "            return obj\n",
        "        # If it's a message, use its content.\n",
        "        if hasattr(obj, \"content\"):\n",
        "            return recursive_convert(obj.content)\n",
        "        # If it's a dict, process each key-value pair.\n",
        "        if isinstance(obj, dict):\n",
        "            return {k: recursive_convert(v) for k, v in obj.items()}\n",
        "        # If it's a list, process each element.\n",
        "        if isinstance(obj, list):\n",
        "            return [recursive_convert(item) for item in obj]\n",
        "        # Otherwise, just return the object.\n",
        "        return obj\n",
        "\n",
        "    converted = recursive_convert(x)\n",
        "    # If the result is a dict or list, return a JSON string.\n",
        "    if isinstance(converted, (dict, list)):\n",
        "        return json.dumps(converted)\n",
        "    return str(converted)\n",
        "\n",
        "convert_to_str = RunnableLambda(convert_output_to_str)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onNd65vJb85S"
      },
      "source": [
        "## Adding Memory to Chains\n",
        "\n",
        "We can't quite add the memory in by itself to the existing chain because the LCEL pipe composition creates a new chain that does not carry over the memory state from your original LLMChain. To get multi-turn behavior where the chain remembers previous inputs, we create the chain with `LLMChain()`, which allows us to add the memory in directly.\n",
        "\n",
        "Also note that we have to then add `{chat_history}` to the prompt so the information contained in memory makes its way back into the system.  However, this has some drawbacks that we will discuss shortly!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6g7lkYrvOzwN",
        "outputId": "d23320fc-d7b9-4da5-835f-621f03002154"
      },
      "outputs": [],
      "source": [
        "system_prompt = SystemMessagePromptTemplate.from_template(\n",
        "    \"You are a helpful assistant. Use the conversation history to inform your answers. Conversation history: {chat_history}\"\n",
        ")\n",
        "human_prompt = HumanMessagePromptTemplate.from_template(\"{input}\")\n",
        "prompt = ChatPromptTemplate.from_messages([system_prompt, human_prompt])\n",
        "\n",
        "chain_with_memory = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=prompt,\n",
        "    memory=memory,\n",
        ")\n",
        "\n",
        "response = chain_with_memory.invoke(\"What is the recipe for mayonnaise?\")\n",
        "print(response['text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utxc_1M-9gHW",
        "outputId": "e38633e5-7e7f-455e-e515-c81d6f95ea18"
      },
      "outputs": [],
      "source": [
        "response = chain_with_memory.invoke(\"What recipe did I just ask you for?\")\n",
        "print(response['text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvNQJnCkcuC3"
      },
      "source": [
        "## Final Comments\n",
        "\n",
        "You might have realized that adding the chat history into the system prompt might become a problem down the road as our conversations with our bot get longer and longer.  This is because your prompt will keep increasing in token size until you either hit the token limit, wind up with a really big LLM bill, or both!  So it is important to think about strategies for only providing a limited amount of information in the history.  You can read about some strategies for limiting the history size [here](https://python.langchain.com/docs/tutorials/chatbot/#managing-conversation-history)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8my0vQtQR4FG"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
