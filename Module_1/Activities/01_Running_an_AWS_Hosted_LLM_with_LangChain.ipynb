{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2v5m0bLLyFV9"
   },
   "source": [
    "# Module 1, Activity 1: Running an AWS-Hosted LLM with LangChain\n",
    "\n",
    "In this demo notebook, we demonstrate how to use the boto3 Python SDK along with the abstractions available through the LangChain package to work with Amazon Bedrock Foundation Models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0HLcJJVZvO3_"
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "from langchain_aws import BedrockLLM\n",
    "from langchain_aws import ChatBedrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v0qxRMSZvae3"
   },
   "outputs": [],
   "source": [
    "session = boto3.session.Session()\n",
    "region = session.region_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Bedrock management connection\n",
    "\n",
    "The Bedrock client is used as the control for Bedrock.  It can do things like list models, check availability, and manage configurations.  But it doesn't actually do anything with the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock = boto3.client(\n",
    "    service_name='bedrock',\n",
    "    region_name = region,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AonF5PGExHQ0"
   },
   "source": [
    "## Listing available models\n",
    "\n",
    "Here we can see which foundation models Bedrock as access to.  However, remember that not all of these are active for this workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dI2Nmbd5vh9G",
    "outputId": "60ded3b7-a1cb-4298-89cc-771f1e21ba5a"
   },
   "outputs": [],
   "source": [
    "[models['modelId'] for models in bedrock.list_foundation_models()['modelSummaries']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N4rEcEt8xZGB"
   },
   "source": [
    "## Creating Bedrock runtime client\n",
    "\n",
    "In this next code block, a dedicated boto3 client for the bedrock-runtime service is created.  This client is responsible for executing runtime operations, such as invoking a model with a given prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "50FnNprfvlEJ"
   },
   "outputs": [],
   "source": [
    "bedrock_runtime = boto3.client(\n",
    "    service_name='bedrock-runtime',\n",
    "    region_name=region,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BMgGTXrcyhu3"
   },
   "source": [
    "## Initializing BedrockLLM and invoking a model\n",
    "\n",
    "Here, the BedrockLLM class from the langchain_aws package is instantiated.  This class serves as a high-level wrapper to interface with AWS-hosted LLMs.\n",
    "The initialization parameters include the model ID (in this case, \"amazon.titan-tg1-large\"), region, and the necessary AWS credentials.  Once the instance is created, the invoke method is used to send a prompt (\"What is the recipe of mayonnaise?\") to the model.  This section demonstrates the fundamental workflow: setting up the model wrapper and making a basic invocation call to test the modelâ€™s response, providing a concrete example of how to interact with AWS-hosted generative AI models using LangChain.\n",
    "\n",
    "Try several different prompts here to see what different types of answers you can get!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "J2fkBhf7vqa7",
    "outputId": "b7a70ec9-ade0-45f2-9d9a-bb1583426124"
   },
   "outputs": [],
   "source": [
    "llm = BedrockLLM(\n",
    "    model_id=\"amazon.titan-tg1-large\",\n",
    "    region_name=region,\n",
    ")\n",
    "\n",
    "llm.invoke(input='What is the recipe of mayonnaise?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eeSlQXzQy2BW"
   },
   "source": [
    "## Introducing ChatBedrock\n",
    "\n",
    "BedrockLLM is designed for single-turn, prompt-based interactions where you provide the prompt (\"What is the recipe for mayonnaise?\") and the model generates an output in one go.  This is fine for simple things, but when you need to have more sophisticated interactions you want something that supports chat-like exchanges where the model can manage context over several turns of dialogue.  Additionally, not all of the available models, including more sophisticated models like Anthropic's Claude 3 Sonnet below, are supported by BedrockLLM.  Hence, we have the more sophisticated ChatBedrock, as shown below.\n",
    "\n",
    "Also note that the output of ChatBedrock contains much more information than just a text output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sOuGMr5bvtgN",
    "outputId": "66f632a6-5340-4af1-c1e6-e9cfa7cc7fef"
   },
   "outputs": [],
   "source": [
    "chat_llm = ChatBedrock(\n",
    "    model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    region_name=region,\n",
    ")\n",
    "\n",
    "chat_llm.invoke(input=\"What is the recipe of mayonnaise?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rjh18jKBz1aQ"
   },
   "source": [
    "## Temperature\n",
    "\n",
    "Temperature is the thing that gives models creativity.  It controls the randomness of the model's responses.  Setting it to 0.0 (the minimum) typically results in a more deterministic and consistent output while setting it to 1.0 (the maximum) results in more creative responses.  Experiment with the temperature setting in the following cell to see how the output changes as a result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pr7xxkXuvx5-",
    "outputId": "fdca4fd7-1ffa-4a02-b4ae-d1db5fb7a4ff"
   },
   "outputs": [],
   "source": [
    "chat_llm = ChatBedrock(\n",
    "    model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    region_name=region,\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "chat_llm.invoke(input=\"What is the recipe of mayonnaise?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FXm4C_hl0RVK"
   },
   "source": [
    "## Limiting the number of tokens returned\n",
    "\n",
    "The cost of using an LLM is dependent on how many tokens are sent back and forth with the model.  The `max_tokens` parameter can provide a limit on how many total tokens are returned.  Limiting the token count can be useful when you need to ensure that the responses remain concise or when working within strict output size constraints.  Experiment with a few different values for this to see how the output changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yQVmJgYgv6tw",
    "outputId": "d9ebffe0-5f9f-44f5-9e5e-b1cc9052721a"
   },
   "outputs": [],
   "source": [
    "chat_llm = ChatBedrock(\n",
    "    model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    region_name=region,\n",
    "    temperature=0.0,\n",
    "    max_tokens=10\n",
    ")\n",
    "\n",
    "chat_llm.invoke(input=\"What is the recipe of mayonnaise?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XbcxV4zIwSDx"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
