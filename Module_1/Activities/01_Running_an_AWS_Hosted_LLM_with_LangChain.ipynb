{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "UNBA_ES_u7Ve",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "54dfcd0c-734d-488e-b7d2-0db84dad48c0"
   },
   "outputs": [],
   "source": [
    "!pip install boto3\n",
    "!pip install langchain\n",
    "!pip install langchain_aws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2v5m0bLLyFV9"
   },
   "source": [
    "# Module 1, Activity 1: Running an AWS-Hosted LLM with LangChain\n",
    "\n",
    "In this demo notebook, we demonstrate how to use the boto3 Python SDK along with the abstractions available through the LangChain package to work with Amazon Bedrock Foundation Models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "0HLcJJVZvO3_"
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "from langchain_aws import BedrockLLM\n",
    "from langchain_aws import ChatBedrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "v0qxRMSZvae3"
   },
   "outputs": [],
   "source": [
    "session = boto3.session.Session()\n",
    "region = session.region_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Bedrock management connection\n",
    "\n",
    "The Bedrock client is used as the control for Bedrock.  It can do things like list models, check availability, and manage configurations.  But it doesn't actually do anything with the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock = boto3.client(\n",
    "    service_name='bedrock',\n",
    "    region_name = region,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AonF5PGExHQ0"
   },
   "source": [
    "## Listing available models\n",
    "\n",
    "Here we can see which foundation models Bedrock as access to.  However, remember that not all of these are active for this workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "dI2Nmbd5vh9G",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "60ded3b7-a1cb-4298-89cc-771f1e21ba5a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['amazon.titan-tg1-large',\n",
       " 'amazon.titan-embed-g1-text-02',\n",
       " 'amazon.titan-text-lite-v1:0:4k',\n",
       " 'amazon.titan-text-lite-v1',\n",
       " 'amazon.titan-text-express-v1:0:8k',\n",
       " 'amazon.titan-text-express-v1',\n",
       " 'amazon.nova-pro-v1:0',\n",
       " 'amazon.nova-lite-v1:0',\n",
       " 'amazon.nova-micro-v1:0',\n",
       " 'amazon.titan-embed-text-v1:2:8k',\n",
       " 'amazon.titan-embed-text-v1',\n",
       " 'amazon.titan-embed-text-v2:0',\n",
       " 'amazon.titan-embed-image-v1:0',\n",
       " 'amazon.titan-embed-image-v1',\n",
       " 'amazon.titan-image-generator-v1:0',\n",
       " 'amazon.titan-image-generator-v1',\n",
       " 'amazon.titan-image-generator-v2:0',\n",
       " 'amazon.rerank-v1:0',\n",
       " 'stability.stable-diffusion-xl-v1:0',\n",
       " 'stability.stable-diffusion-xl-v1',\n",
       " 'stability.sd3-large-v1:0',\n",
       " 'stability.sd3-5-large-v1:0',\n",
       " 'stability.stable-image-core-v1:0',\n",
       " 'stability.stable-image-core-v1:1',\n",
       " 'stability.stable-image-ultra-v1:0',\n",
       " 'stability.stable-image-ultra-v1:1',\n",
       " 'anthropic.claude-3-5-sonnet-20241022-v2:0:18k',\n",
       " 'anthropic.claude-3-5-sonnet-20241022-v2:0:51k',\n",
       " 'anthropic.claude-3-5-sonnet-20241022-v2:0:200k',\n",
       " 'anthropic.claude-3-5-sonnet-20241022-v2:0',\n",
       " 'anthropic.claude-3-7-sonnet-20250219-v1:0',\n",
       " 'anthropic.claude-3-5-haiku-20241022-v1:0',\n",
       " 'anthropic.claude-instant-v1:2:100k',\n",
       " 'anthropic.claude-instant-v1',\n",
       " 'anthropic.claude-v2:0:18k',\n",
       " 'anthropic.claude-v2:0:100k',\n",
       " 'anthropic.claude-v2:1:18k',\n",
       " 'anthropic.claude-v2:1:200k',\n",
       " 'anthropic.claude-v2:1',\n",
       " 'anthropic.claude-v2',\n",
       " 'anthropic.claude-3-sonnet-20240229-v1:0:28k',\n",
       " 'anthropic.claude-3-sonnet-20240229-v1:0:200k',\n",
       " 'anthropic.claude-3-sonnet-20240229-v1:0',\n",
       " 'anthropic.claude-3-haiku-20240307-v1:0:48k',\n",
       " 'anthropic.claude-3-haiku-20240307-v1:0:200k',\n",
       " 'anthropic.claude-3-haiku-20240307-v1:0',\n",
       " 'anthropic.claude-3-opus-20240229-v1:0:12k',\n",
       " 'anthropic.claude-3-opus-20240229-v1:0:28k',\n",
       " 'anthropic.claude-3-opus-20240229-v1:0:200k',\n",
       " 'anthropic.claude-3-opus-20240229-v1:0',\n",
       " 'anthropic.claude-3-5-sonnet-20240620-v1:0:18k',\n",
       " 'anthropic.claude-3-5-sonnet-20240620-v1:0:51k',\n",
       " 'anthropic.claude-3-5-sonnet-20240620-v1:0:200k',\n",
       " 'anthropic.claude-3-5-sonnet-20240620-v1:0',\n",
       " 'cohere.command-text-v14:7:4k',\n",
       " 'cohere.command-text-v14',\n",
       " 'cohere.command-r-v1:0',\n",
       " 'cohere.command-r-plus-v1:0',\n",
       " 'cohere.command-light-text-v14:7:4k',\n",
       " 'cohere.command-light-text-v14',\n",
       " 'cohere.embed-english-v3:0:512',\n",
       " 'cohere.embed-english-v3',\n",
       " 'cohere.embed-multilingual-v3:0:512',\n",
       " 'cohere.embed-multilingual-v3',\n",
       " 'cohere.rerank-v3-5:0',\n",
       " 'deepseek.r1-v1:0',\n",
       " 'meta.llama3-8b-instruct-v1:0',\n",
       " 'meta.llama3-70b-instruct-v1:0',\n",
       " 'meta.llama3-1-8b-instruct-v1:0:128k',\n",
       " 'meta.llama3-1-8b-instruct-v1:0',\n",
       " 'meta.llama3-1-70b-instruct-v1:0:128k',\n",
       " 'meta.llama3-1-70b-instruct-v1:0',\n",
       " 'meta.llama3-1-405b-instruct-v1:0',\n",
       " 'meta.llama3-2-11b-instruct-v1:0:128k',\n",
       " 'meta.llama3-2-11b-instruct-v1:0',\n",
       " 'meta.llama3-2-90b-instruct-v1:0:128k',\n",
       " 'meta.llama3-2-90b-instruct-v1:0',\n",
       " 'meta.llama3-2-1b-instruct-v1:0:128k',\n",
       " 'meta.llama3-2-1b-instruct-v1:0',\n",
       " 'meta.llama3-2-3b-instruct-v1:0:128k',\n",
       " 'meta.llama3-2-3b-instruct-v1:0',\n",
       " 'meta.llama3-3-70b-instruct-v1:0',\n",
       " 'mistral.mistral-7b-instruct-v0:2',\n",
       " 'mistral.mixtral-8x7b-instruct-v0:1',\n",
       " 'mistral.mistral-large-2402-v1:0',\n",
       " 'mistral.mistral-large-2407-v1:0',\n",
       " 'luma.ray-v2:0']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[models['modelId'] for models in bedrock.list_foundation_models()['modelSummaries']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N4rEcEt8xZGB"
   },
   "source": [
    "## Creating Bedrock runtime client\n",
    "\n",
    "In this next code block, a dedicated boto3 client for the bedrock-runtime service is created.  This client is responsible for executing runtime operations, such as invoking a model with a given prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "50FnNprfvlEJ"
   },
   "outputs": [],
   "source": [
    "bedrock_runtime = boto3.client(\n",
    "    service_name='bedrock-runtime',\n",
    "    region_name=region,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BMgGTXrcyhu3"
   },
   "source": [
    "## Initializing BedrockLLM and invoking a model\n",
    "\n",
    "Here, the BedrockLLM class from the langchain_aws package is instantiated.  This class serves as a high-level wrapper to interface with AWS-hosted LLMs.\n",
    "The initialization parameters include the model ID (in this case, \"amazon.titan-tg1-large\"), region, and the necessary AWS credentials.  Once the instance is created, the invoke method is used to send a prompt (\"What is the recipe of mayonnaise?\") to the model.  This section demonstrates the fundamental workflow: setting up the model wrapper and making a basic invocation call to test the modelâ€™s response, providing a concrete example of how to interact with AWS-hosted generative AI models using LangChain.\n",
    "\n",
    "Try several different prompts here to see what different types of answers you can get!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "J2fkBhf7vqa7",
    "outputId": "b7a70ec9-ade0-45f2-9d9a-bb1583426124"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nHere is the recipe of mayonnaise:\\n\\nIngredients:\\n- 2 egg yolks\\n- 1 tablespoon Dijon mustard\\n- 1 tablespoon lemon juice\\n- 1 teaspoon salt\\n- 1 cup vegetable oil\\n\\nInstructions:\\n1. In a blender or food processor, combine the egg yolks, mustard, lemon juice, and salt. Blend until smooth.\\n2. With the blender running, slowly drizzle in the vegetable oil until the mixture thickens and forms a thick mayonnaise.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = BedrockLLM(\n",
    "    model_id=\"amazon.titan-tg1-large\",\n",
    "    region_name=region,\n",
    ")\n",
    "\n",
    "llm.invoke(input='What is the recipe of mayonnaise?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eeSlQXzQy2BW"
   },
   "source": [
    "## Introducing ChatBedrock\n",
    "\n",
    "BedrockLLM is designed for single-turn, prompt-based interactions where you provide the prompt (\"What is the recipe for mayonnaise?\") and the model generates an output in one go.  This is fine for simple things, but when you need to have more sophisticated interactions you want something that supports chat-like exchanges where the model can manage context over several turns of dialogue.  Additionally, not all of the available models, including more sophisticated models like Anthropic's Claude 3 Sonnet below, are supported by BedrockLLM.  Hence, we have the more sophisticated ChatBedrock, as shown below.\n",
    "\n",
    "Also note that the output of ChatBedrock contains much more information than just a text output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sOuGMr5bvtgN",
    "outputId": "66f632a6-5340-4af1-c1e6-e9cfa7cc7fef"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Here is a basic recipe for homemade mayonnaise:\\n\\nIngredients:\\n- 1 egg yolk\\n- 1 tablespoon lemon juice or white wine vinegar\\n- 1 teaspoon Dijon mustard (optional)\\n- 1/4 teaspoon salt\\n- 3/4 cup vegetable oil or mild olive oil\\n\\nInstructions:\\n\\n1. In a medium bowl, whisk together the egg yolk, lemon juice/vinegar, mustard (if using), and salt.\\n\\n2. Very slowly, while whisking constantly, start adding the oil a few drops at a time. Once an emulsion starts to form, you can add the oil in a very thin steady stream, whisking vigorously the whole time.\\n\\n3. Continue whisking and adding oil until all the oil is incorporated and the mayonnaise has thickened to your desired consistency. \\n\\n4. Taste and adjust seasoning if needed by adding more lemon juice, vinegar or salt.\\n\\n5. Transfer to an airtight container and refrigerate for up to 1 week.\\n\\nThe key is adding the oil very slowly while whisking constantly to allow a stable emulsion to form between the egg yolk and oil. This creates the thick, creamy mayonnaise texture.', additional_kwargs={'usage': {'prompt_tokens': 16, 'completion_tokens': 287, 'total_tokens': 303}, 'stop_reason': 'end_turn', 'model_id': 'anthropic.claude-3-sonnet-20240229-v1:0'}, response_metadata={'usage': {'prompt_tokens': 16, 'completion_tokens': 287, 'total_tokens': 303}, 'stop_reason': 'end_turn', 'model_id': 'anthropic.claude-3-sonnet-20240229-v1:0'}, id='run-684e41de-706b-45c9-914f-557cf59ac8c4-0', usage_metadata={'input_tokens': 16, 'output_tokens': 287, 'total_tokens': 303})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_llm = ChatBedrock(\n",
    "    model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    region_name=region,\n",
    ")\n",
    "\n",
    "chat_llm.invoke(input=\"What is the recipe of mayonnaise?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rjh18jKBz1aQ"
   },
   "source": [
    "## Temperature\n",
    "\n",
    "Temperature is the thing that gives models creativity.  It controls the randomness of the model's responses.  Setting it to 0.0 (the minimum) typically results in a more deterministic and consistent output while setting it to 1.0 (the maximum) results in more creative responses.  Experiment with the temperature setting in the following cell to see how the output changes as a result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pr7xxkXuvx5-",
    "outputId": "fdca4fd7-1ffa-4a02-b4ae-d1db5fb7a4ff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Here is a basic recipe for homemade mayonnaise:\\n\\nIngredients:\\n- 1 egg yolk\\n- 1 tablespoon lemon juice or white wine vinegar\\n- 1/2 teaspoon Dijon mustard (optional)\\n- 1/4 teaspoon salt\\n- 3/4 cup vegetable oil or mild olive oil\\n\\nInstructions:\\n\\n1. In a medium bowl, whisk together the egg yolk, lemon juice/vinegar, mustard (if using), and salt.\\n\\n2. Very slowly, while whisking constantly, drizzle in the oil just a few drops at a time at first. This allows the mixture to emulsify properly.\\n\\n3. Once about 1/4 cup of oil has been incorporated, you can start adding the oil in a thin steady stream while whisking vigorously. \\n\\n4. Continue whisking and adding oil until all the oil is incorporated and the mayonnaise has thickened to your desired consistency.\\n\\n5. Taste and adjust seasoning if needed by adding more lemon juice, salt, etc.\\n\\n6. Transfer to an airtight container and refrigerate for up to 1 week.\\n\\nThe key is adding the oil very slowly at first while whisking constantly to allow a stable emulsion to form. This gives mayonnaise its thick, creamy texture.', additional_kwargs={'usage': {'prompt_tokens': 16, 'completion_tokens': 304, 'total_tokens': 320}, 'stop_reason': 'end_turn', 'model_id': 'anthropic.claude-3-sonnet-20240229-v1:0'}, response_metadata={'usage': {'prompt_tokens': 16, 'completion_tokens': 304, 'total_tokens': 320}, 'stop_reason': 'end_turn', 'model_id': 'anthropic.claude-3-sonnet-20240229-v1:0'}, id='run-6d25bd0c-8794-4bd4-aa67-ac62621d7a00-0', usage_metadata={'input_tokens': 16, 'output_tokens': 304, 'total_tokens': 320})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_llm = ChatBedrock(\n",
    "    model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    region_name=region,\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "chat_llm.invoke(input=\"What is the recipe of mayonnaise?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FXm4C_hl0RVK"
   },
   "source": [
    "## Limiting the number of tokens returned\n",
    "\n",
    "The cost of using an LLM is dependent on how many tokens are sent back and forth with the model.  The `max_tokens` parameter can provide a limit on how many total tokens are returned.  Limiting the token count can be useful when you need to ensure that the responses remain concise or when working within strict output size constraints.  Experiment with a few different values for this to see how the output changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yQVmJgYgv6tw",
    "outputId": "d9ebffe0-5f9f-44f5-9e5e-b1cc9052721a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Here is a basic recipe for homemade may', additional_kwargs={'usage': {'prompt_tokens': 16, 'completion_tokens': 10, 'total_tokens': 26}, 'stop_reason': 'max_tokens', 'model_id': 'anthropic.claude-3-sonnet-20240229-v1:0'}, response_metadata={'usage': {'prompt_tokens': 16, 'completion_tokens': 10, 'total_tokens': 26}, 'stop_reason': 'max_tokens', 'model_id': 'anthropic.claude-3-sonnet-20240229-v1:0'}, id='run-1f4ae27f-263d-4c97-bd83-e357a559802e-0', usage_metadata={'input_tokens': 16, 'output_tokens': 10, 'total_tokens': 26})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_llm = ChatBedrock(\n",
    "    model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    region_name=region,\n",
    "    temperature=0.0,\n",
    "    max_tokens=10\n",
    ")\n",
    "\n",
    "chat_llm.invoke(input=\"What is the recipe of mayonnaise?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XbcxV4zIwSDx"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
