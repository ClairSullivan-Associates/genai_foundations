{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1fb359d-bd68-4dea-99fb-5d5764a00f3a",
   "metadata": {},
   "source": [
    "# Module 1, Activity 3: Connecting AWS Services to an LLM Chain\n",
    "\n",
    "In this notebook we will see some basic pipelines connecting various AWS services to Bedrock for using in an LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d360bc-4dfb-4e45-b9d8-9dda50e2c6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "import boto3\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_aws import ChatBedrock\n",
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "from langchain_core.runnables import RunnableLambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5251b1fc-3e1d-463b-8441-1555a078170e",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.session.Session()\n",
    "region = session.region_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa480a0-d9fd-42b2-89c3-46a0a7fe0941",
   "metadata": {},
   "source": [
    "## About this cell\n",
    "\n",
    "This is a helper function to download data from a file in an S3 bucket.  You can also make the connection directly, if you choose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45338698-94d3-4f91-a3f3-b87426d1d698",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_s3(bucket_name, key):\n",
    "    s3 = boto3.client(\n",
    "        's3',\n",
    "        region_name=region,\n",
    "    )\n",
    "    response = s3.get_object(Bucket=bucket_name, Key=key)\n",
    "    data = response['Body'].read().decode('utf-8')\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6f8673-470f-4235-9498-32e746337e31",
   "metadata": {},
   "source": [
    "We now make the connection to the bucket and pull down a specific text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2df0d5-4e44-4179-a804-f2ca512bd1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_data = get_data_from_s3(\"...\", \"constitution.txt\")\n",
    "s3_data[0:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5398d40-ef70-46c1-b541-ce676b948c16",
   "metadata": {},
   "source": [
    "## The prompt\n",
    "\n",
    "Here we are creating our system and human prompts, as before.  Notice that we have provided the system prompt with a bit of instruction on what it is expected to do, namely summarize the provided text.  This text is passed in through the `s3_data` variable in the human prompt.  Again, don't forget that data that you pass in takes up tokens and must fit within the context window.  Anything that doesn't fit will not be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2e3d95-e542-4a8f-8087-6a432659f56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = SystemMessagePromptTemplate.from_template(\n",
    "    \"You are a helpful assistant that specializes in summarizing documents.\"\n",
    ")\n",
    "human_prompt = HumanMessagePromptTemplate.from_template(\n",
    "    \"Summarize the following document concisely:\\n\\n{s3_data}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([system_prompt, human_prompt])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303df3d7-71a8-4b06-b095-43aa2fe440af",
   "metadata": {},
   "source": [
    "# The chain\n",
    "\n",
    "As before, we are creating our chain using LCEL of the prompt, the model, and the output parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b037d3-92cf-4d67-a33b-030a41007952",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatBedrock(\n",
    "    model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    region_name=region,\n",
    "    temperature=0.0,\n",
    "    max_tokens=200,\n",
    ")\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9234db5-c832-46df-b4a7-0e1c646f7a0e",
   "metadata": {},
   "source": [
    "## Running the chain\n",
    "\n",
    "Notice that in this case we are passing the data into the variable `s3_data` for use in the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2ce24f-9f85-4945-af71-a296864f035e",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = chain.invoke({\"s3_data\": s3_data})\n",
    "print(\"Summary:\", summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3401d0e-4f80-4c22-a71e-6e774c4324e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
